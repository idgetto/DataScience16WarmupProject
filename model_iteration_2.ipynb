{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Iteration 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this iteration of predicting survival rates for the kaggle competition, [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic), I will try out a few more machine learning models. While doing this, I hope to learn why one algorithm is more effective than other for this problem. I'll also try to explain the tradeoffs of picking one model over another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, I will be exploring the following three models:\n",
    "1. Random Forests\n",
    "2. Boosting\n",
    "3. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "predictors = ['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "\n",
    "def clean_data(data_frame, predictors):\n",
    "    # convert Sex to binary, (male = 0, female = 1, )\n",
    "    data_frame.loc[data_frame.Sex == 'male', 'Sex'] = 0\n",
    "    data_frame.loc[data_frame.Sex == 'female', 'Sex'] = 1\n",
    "    \n",
    "    # convert Embarked to S = 0, C = 1, Q = 2    \n",
    "    data_frame.loc[data_frame.Embarked == 'S', 'Embarked'] = 0\n",
    "    data_frame.loc[data_frame.Embarked == 'C', 'Embarked'] = 1\n",
    "    data_frame.loc[data_frame.Embarked == 'Q', 'Embarked'] = 2\n",
    "\n",
    "    # fill all NaN's with the median value for that feature\n",
    "    for predictor in predictors:\n",
    "        data_frame[predictor] = data_frame[predictor].fillna(data_frame[predictor].median())\n",
    "        \n",
    "clean_data(train, predictors)\n",
    "clean_data(test, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's a Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Random Forest** model is a learning method that can be used for classification as well as regression. In our case we'll use the classification functionality because we want to predict the survival of passengers aboard the titanic. \n",
    "\n",
    "So how does the **Random Forest** model actually classify our data? This model relies on another machine learning model called **Decision Trees** to perform classifications and regressions. Since we're using a bunch of decision *trees*, we end up with a random *forest*. Because **Random Forests** uses another model, we call **Random Forests** an *ensemble* learner. It works by using an ensemble, or collection, of other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "\n",
    "Let's talk about **Decision Trees**. As the underlying model for **Random Forests**, **Decision Trees** must also be able to perform classifications and regressions. Here's what a decision tree might look like: \n",
    "\n",
    "![Titanic Decision Tree Image](img/decision_tree.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the top of the diagram, we begin with all the passengers. If a passenger's sex is female(Sex=1) then they are sorted towards the right side of the tree, and males(Sex=0) are sorted toward the left side. Males are then sorted based on their age. Men younger than 6.5 filter into the next left node and men older than 6.5 filter into the lower node. We continue sorting the passengers based on similar questions. Once passenger's make their way to the lowest tier of nodes, their class is decided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Trees** work like we just saw above. They ask a series of question and sort the passenger's into categories based on the answer to those questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest is an Ensemble of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is created by generating a number of decision trees. Each of these decision trees is created using a random subset of the input data. That's where the *random* in random forests comes from. The decision trees then predict a class for a given passenger. Some may predict *Survived* and some might predict *Died*, but what's important is the majority vote. If two trees think Jack survived and three predict Jack died, then we go with the conclusion that Jack sadly died."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Random Forests\n",
    "\n",
    "* Classification or Regression problems\n",
    "* Noisy inputs\n",
    "* Many features of unknown importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scikit-learn, using a random forest learner is simple. There are some optoional parameters that can affect the performance of the classifier though. The ones I've found to be more important are **max_depth**, the maximum depth of any of the decision trees used, and **n_estimators**, the number of decision trees to use. It is probably worthwhile to adjust these parameters to get the best score you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score: 0.804714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=4)\n",
    "scores = cross_val_score(clf, train[predictors], train.Survived, cv=3)\n",
    "print \"Cross Validation Score: %f\" % scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kaggle Score](img/kaggle_random_forest.png)\n",
    "\n",
    "Kaggle Score: **0.78947**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "* https://en.wikipedia.org/wiki/Random_forest\n",
    "* https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Random Forests, **Boosting** models are ensemble learners. This means that models like AdaBoost rely on other underlying learners. In the case of Boosting, many weak learners are used together to achieve a strong learner. \n",
    "\n",
    "Boosting works much like a bunch of struggling students in a math class. Let's say each student can score 60% on their test independently. If each student is better at answering different questions on the test, then maybe as a group all of the students would be able to bring their separate pockets of knowledge together and answer all of the questions correctly. Boosting works the same way. We take a bunch of learners that have different areas of expertise and get them to work together and answer the questions each learner knows best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Boosting\n",
    "\n",
    "* Well labeled data\n",
    "* Low noise\n",
    "* Few outliers present\n",
    "* As a first pass model\n",
    "* High dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score: 0.797980\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "scores = cross_val_score(clf, train[predictors], train.Survived, cv=3)\n",
    "print \"Cross Validation Score: %f\" % scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kaggle Score](img/kaggle_adaboost.png)\n",
    "\n",
    "Kaggle Score: **0.75120**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Boosting_%28machine_learning%29\n",
    "* http://www.cs.princeton.edu/~schapire/talks/picasso-minicourse.pdf\n",
    "* http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is learning model that linearly separates data into classes. The following image shows three possible lines the SVM could draw to separate the black dots from the white dots. H1 doesn't actually separate the data, this would not be chosen. H2 does separate the data into the correct classes, but it leaves little margin. Finally, H3 both separates the data and leaves ample margin. H3 would be the most effective separator. ![SVM](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Svm_separating_hyperplanes_%28SVG%29.svg/330px-Svm_separating_hyperplanes_%28SVG%29.svg.png) By User:ZackWeinberg, based on PNG version by User:Cyc [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use SVM\n",
    "\n",
    "* High dimensional data\n",
    "* Slightly more dimensions than data points\n",
    "* Probability of classifications are not needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score: 0.691358\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "scores = cross_val_score(clf, train[predictors], train.Survived, cv=3)\n",
    "print \"Cross Validation Score: %f\" % scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kaggle Score](img/kaggle_svc.png)\n",
    "\n",
    "Kaggle Score: **0.62679**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* http://www.cs.princeton.edu/~schapire/talks/picasso-minicourse.pdf\n",
    "* https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "* http://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "\n",
    "♪ all together now... ♫ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(max_depth=4)\n",
    "boost = AdaBoostClassifier()\n",
    "svc = SVC()\n",
    "\n",
    "forest.fit(train[predictors], train.Survived)\n",
    "boost.fit(train[predictors], train.Survived)\n",
    "svc.fit(train[predictors], train.Survived)\n",
    "\n",
    "y_forest = forest.predict(test[predictors])\n",
    "y_boost = boost.predict(test[predictors])\n",
    "y_svc = svc.predict(test[predictors])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'forest': y_forest,\n",
    "    'boost': y_boost,\n",
    "    'svc': y_svc\n",
    "})\n",
    "\n",
    "df['mode'] = df.mode(axis=1)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'PassengerId': test.PassengerId,\n",
    "    'Survived': df['mode']\n",
    "})\n",
    "results.to_csv('ensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kaggle Score](img/kaggle_ensemble.png)\n",
    "\n",
    "Kaggle Score: 0.74641"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
